{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2cd1b06",
   "metadata": {},
   "source": [
    "# Remote Work Tracker BI Project: ETL, DB, and Utility Scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a2440b",
   "metadata": {},
   "source": [
    "This Jupyter Notebook provides a comprehensive overview and demonstration of the Extract, Transform, Load (ETL) process, database interaction, and utility functions developed for the Remote Work Tracker Business Intelligence (BI) project. These scripts are designed to process raw job data (e.g., from the Remotive.com API) and store it in a structured SQLite database, making it ready for further analysis and visualization in tools like Power BI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761eb29b",
   "metadata": {},
   "source": [
    "## Project Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa18d4ca",
   "metadata": {},
   "source": [
    "The core components covered in this notebook are:\n",
    "1.  **`db_schema_and_etl_design.md`**: Documentation outlining the database schema and ETL process.\n",
    "2.  **`db_connector.py`**: Python script for connecting to and interacting with the SQLite database.\n",
    "3.  **`etl_script.py`**: Python script implementing the Extract, Transform, Load logic.\n",
    "4.  **`utils.py`**: Python script containing general utility functions, such as logging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f948b300",
   "metadata": {},
   "source": [
    "## 1. Database Schema Design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c61ecee",
   "metadata": {},
   "source": [
    "The foundation of our BI project is a well-defined database schema. We are using a simple SQLite database with a single table, `remote_jobs`, to store the processed job data. The schema is designed to capture all relevant information from the job postings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6870fb",
   "metadata": {},
   "source": [
    "### `remote_jobs` Table Structure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a215442d",
   "metadata": {},
   "source": [
    "| Column Name                   | Data Type   | Constraints       | Description                                      |\n",
    "| :---------------------------- | :---------- | :---------------- | :----------------------------------------------- |\n",
    "| `id`                          | INTEGER     | PRIMARY KEY       | Unique identifier for each job posting (from API)|\n",
    "| `job_title`                   | TEXT        | NOT NULL          | Title of the job                                 |\n",
    "| `company_name`                | TEXT        | NOT NULL          | Name of the hiring company                       |\n",
    "| `company_name`                | TEXT        | NOT NULL          | Name of the hiring company                       |\n",
    "| `publication_date`            | TEXT        | NOT NULL          | Date and time the job was published (ISO format) |\n",
    "| `job_type`                    | TEXT        |                   | Type of employment (e.g., full_time, contract)   |\n",
    "| `category`                    | TEXT        |                   | Job category (e.g., Software Development)        |\n",
    "| `candidate_required_location` | TEXT        |                   | Geographical restrictions for candidates         |\n",
    "| `salary_range`                | TEXT        |                   | Stated salary range, if available                |\n",
    "| `job_description`             | TEXT        |                   | Full description of the job                      |\n",
    "| `source_url`                  | TEXT        | UNIQUE, NOT NULL  | URL to the original job posting                  |\n",
    "| `company_logo`                | TEXT        |                   | URL to the company logo                          |\n",
    "| `job_board`                   | TEXT        | NOT NULL          | Source job board (e.g., Remotive.com)            |\n",
    "| `ingestion_timestamp`         | TIMESTAMP   | DEFAULT CURRENT_TIMESTAMP | Timestamp when the record was ingested   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acacc7f2",
   "metadata": {},
   "source": [
    "*Note: The `db_schema_and_etl_design.md` file contains a more detailed explanation of the schema and ETL process.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310579a3",
   "metadata": {},
   "source": [
    "## 2. Database Connector (`db_connector.py`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de169bc",
   "metadata": {},
   "source": [
    "The `db_connector.py` script provides a `DBConnector` class to manage interactions with the SQLite database. It handles connecting, disconnecting, creating the `remote_jobs` table, and inserting job data from a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87d4fa5",
   "metadata": {},
   "source": [
    "### Key Features:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b50c87",
   "metadata": {},
   "source": [
    "- **Connection Management**: `connect()` and `disconnect()` methods for robust database handling.\n",
    "- **Schema Initialization**: `create_table()` ensures the `remote_jobs` table exists with the defined schema.\n",
    "- **Data Insertion**: `insert_jobs(df)` efficiently inserts DataFrame records, using `INSERT OR IGNORE` to prevent duplicate entries based on the `id` column (which is the primary key from the API). This is crucial for handling daily scrapes without re-inserting old data.\n",
    "- **Data Retrieval**: `fetch_all_jobs()` allows for easy retrieval of all stored job data into a Pandas DataFrame for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305a1dce",
   "metadata": {},
   "source": [
    "### Code (`db_connector.py`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48b750fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to database: remote_jobs.db\n",
      "Table 'remote_jobs' ensured to exist.\n",
      "Successfully inserted/ignored 2 records into 'remote_jobs'.\n",
      "\n",
      "All jobs in DB:\n",
      "   id          job_title        company_name     publication_date   job_type  \\\n",
      "0   1  Software Engineer           Tech Corp  2025-10-15T10:00:00  full_time   \n",
      "1   2       Data Analyst  Data Insights Inc.  2025-10-14T11:30:00   contract   \n",
      "\n",
      "               category candidate_required_location        salary_range  \\\n",
      "0  Software Development                   Worldwide  $80,000 - $120,000   \n",
      "1         Data Analysis                   Remote US     $50/hr - $70/hr   \n",
      "\n",
      "                  job_description               source_url  \\\n",
      "0  Develop and maintain software.  http://example.com/job1   \n",
      "1         Analyze large datasets.  http://example.com/job2   \n",
      "\n",
      "                   company_logo    job_board         ingestion_timestamp  \n",
      "0  http://example.com/logo1.png  ExampleJobs  2025-10-18T19:41:30.210673  \n",
      "1  http://example.com/logo2.png  ExampleJobs  2025-10-18T19:41:30.210682  \n",
      "Disconnected from database.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "class DBConnector:\n",
    "    def __init__(self, db_name=\"remote_jobs.db\"):\n",
    "        self.db_name = db_name\n",
    "        self.conn = None\n",
    "        self.cursor = None\n",
    "\n",
    "    def connect(self):\n",
    "        \"\"\"Establishes a connection to the SQLite database.\"\"\"\n",
    "        try:\n",
    "            self.conn = sqlite3.connect(self.db_name)\n",
    "            self.cursor = self.conn.cursor()\n",
    "            print(f\"Connected to database: {self.db_name}\")\n",
    "        except sqlite3.Error as e:\n",
    "            print(f\"Error connecting to database: {e}\")\n",
    "\n",
    "    def disconnect(self):\n",
    "        \"\"\"Closes the database connection.\"\"\"\n",
    "        if self.conn:\n",
    "            self.conn.close()\n",
    "            print(\"Disconnected from database.\")\n",
    "\n",
    "    def create_table(self):\n",
    "        \"\"\"Creates the remote_jobs table if it doesn't exist.\"\"\"\n",
    "        if not self.conn:\n",
    "            self.connect()\n",
    "        \n",
    "        create_table_sql = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS remote_jobs (\n",
    "            id INTEGER PRIMARY KEY,\n",
    "            job_title TEXT NOT NULL,\n",
    "            company_name TEXT NOT NULL,\n",
    "            publication_date TEXT NOT NULL,\n",
    "            job_type TEXT,\n",
    "            category TEXT,\n",
    "            candidate_required_location TEXT,\n",
    "            salary_range TEXT,\n",
    "            job_description TEXT,\n",
    "            source_url TEXT UNIQUE NOT NULL,\n",
    "            company_logo TEXT,\n",
    "            job_board TEXT NOT NULL,\n",
    "            ingestion_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        );\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.cursor.execute(create_table_sql)\n",
    "            self.conn.commit()\n",
    "            print(\"Table 'remote_jobs' ensured to exist.\")\n",
    "        except sqlite3.Error as e:\n",
    "            print(f\"Error creating table: {e}\")\n",
    "\n",
    "    def insert_jobs(self, df: pd.DataFrame):\n",
    "        \"\"\"Inserts job data from a Pandas DataFrame into the remote_jobs table.\n",
    "           Handles duplicates by ignoring entries with existing source_url.\n",
    "        \"\"\"\n",
    "        if df.empty:\n",
    "            print(\"No data to insert.\")\n",
    "            return\n",
    "\n",
    "        if not self.conn:\n",
    "            self.connect()\n",
    "            self.create_table() # Ensure table exists before inserting\n",
    "\n",
    "        # Prepare data for insertion\n",
    "        # Convert DataFrame rows to a list of tuples, ensuring order matches SQL INSERT statement\n",
    "        # And handle potential None values for columns that can be null\n",
    "        data_to_insert = []\n",
    "        for index, row in df.iterrows():\n",
    "            data_to_insert.append((\n",
    "                row.get(\"id\"),\n",
    "                row.get(\"job_title\"),\n",
    "                row.get(\"company_name\"),\n",
    "                row.get(\"publication_date\"),\n",
    "                row.get(\"job_type\"),\n",
    "                row.get(\"category\"),\n",
    "                row.get(\"candidate_required_location\"),\n",
    "                row.get(\"salary_range\"),\n",
    "                row.get(\"job_description\"),\n",
    "                row.get(\"source_url\"),\n",
    "                row.get(\"company_logo\"),\n",
    "                row.get(\"job_board\"),\n",
    "                row.get(\"ingestion_timestamp\", datetime.now().isoformat()) # Default if not set by ETL\n",
    "            ))\n",
    "\n",
    "        insert_sql = \"\"\"\n",
    "        INSERT OR IGNORE INTO remote_jobs (\n",
    "            id, job_title, company_name, publication_date, job_type, category,\n",
    "            candidate_required_location, salary_range, job_description, source_url,\n",
    "            company_logo, job_board, ingestion_timestamp\n",
    "        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?);\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.cursor.executemany(insert_sql, data_to_insert)\n",
    "            self.conn.commit()\n",
    "            print(f\"Successfully inserted/ignored {len(data_to_insert)} records into 'remote_jobs'.\")\n",
    "        except sqlite3.Error as e:\n",
    "            print(f\"Error inserting data: {e}\")\n",
    "\n",
    "    def fetch_all_jobs(self):\n",
    "        \"\"\"Fetches all job records from the database.\"\"\"\n",
    "        if not self.conn:\n",
    "            self.connect()\n",
    "\n",
    "        try:\n",
    "            self.cursor.execute(\"SELECT * FROM remote_jobs;\")\n",
    "            columns = [description[0] for description in self.cursor.description]\n",
    "            rows = self.cursor.fetchall()\n",
    "            df = pd.DataFrame(rows, columns=columns)\n",
    "            return df\n",
    "        except sqlite3.Error as e:\n",
    "            print(f\"Error fetching data: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    db = DBConnector()\n",
    "    db.connect()\n",
    "    db.create_table()\n",
    "\n",
    "    # Example of inserting dummy data\n",
    "    dummy_data = pd.DataFrame([\n",
    "        {\n",
    "            \"id\": 1,\n",
    "            \"job_title\": \"Software Engineer\",\n",
    "            \"company_name\": \"Tech Corp\",\n",
    "            \"publication_date\": \"2025-10-15T10:00:00\",\n",
    "            \"job_type\": \"full_time\",\n",
    "            \"category\": \"Software Development\",\n",
    "            \"candidate_required_location\": \"Worldwide\",\n",
    "            \"salary_range\": \"$80,000 - $120,000\",\n",
    "            \"job_description\": \"Develop and maintain software.\",\n",
    "            \"source_url\": \"http://example.com/job1\",\n",
    "            \"company_logo\": \"http://example.com/logo1.png\",\n",
    "            \"job_board\": \"ExampleJobs\",\n",
    "            \"ingestion_timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        {\n",
    "            \"id\": 2,\n",
    "            \"job_title\": \"Data Analyst\",\n",
    "            \"company_name\": \"Data Insights Inc.\",\n",
    "            \"publication_date\": \"2025-10-14T11:30:00\",\n",
    "            \"job_type\": \"contract\",\n",
    "            \"category\": \"Data Analysis\",\n",
    "            \"candidate_required_location\": \"Remote US\",\n",
    "            \"salary_range\": \"$50/hr - $70/hr\",\n",
    "            \"job_description\": \"Analyze large datasets.\",\n",
    "            \"source_url\": \"http://example.com/job2\",\n",
    "            \"company_logo\": \"http://example.com/logo2.png\",\n",
    "            \"job_board\": \"ExampleJobs\",\n",
    "            \"ingestion_timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "    ])\n",
    "    db.insert_jobs(dummy_data)\n",
    "\n",
    "    # Fetch and display data\n",
    "    all_jobs = db.fetch_all_jobs()\n",
    "    print(\"\\nAll jobs in DB:\")\n",
    "    print(all_jobs.head())\n",
    "\n",
    "    db.disconnect()\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c6c8af",
   "metadata": {},
   "source": [
    "## 3. ETL Script (`etl_script.py`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1ef9f6",
   "metadata": {},
   "source": [
    "The `etl_script.py` orchestrates the data pipeline, performing the Extract, Transform, and Load operations. It reads raw data, cleans and standardizes it, and then uses the `DBConnector` to persist it in the database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c3e619",
   "metadata": {},
   "source": [
    "### Key Functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3ed5fa",
   "metadata": {},
   "source": [
    "- **`extract_data(file_path)`**: Reads the raw job data from a specified CSV file into a Pandas DataFrame. This acts as the \"Extract\" stage.\n",
    "- **`transform_data(df)`**: Cleans and transforms the DataFrame. This includes renaming columns to match the database schema, handling missing values (replacing `NaN` with `None`), converting `publication_date` to a consistent ISO format, and adding an `ingestion_timestamp`. This is the \"Transform\" stage.\n",
    "- **`load_data(df, db_connector)`**: Takes the transformed DataFrame and an instance of `DBConnector` to insert the data into the `remote_jobs` table. This is the \"Load\" stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ec5153",
   "metadata": {},
   "source": [
    "### Code (`etl_script.py`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ad884d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_data(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Extracts raw job data from a CSV file into a Pandas DataFrame.\"\"\"\n",
    "    try:\n",
    "     df = pd.read_csv(r'd:\\courses\\Data Science\\Projects\\Python\\remote-work-tracker\\data\\raw\\remotive_jobs_extended.csv')\n",
    "     print(f\"Successfully extracted {len(df)} records from {file_path}\")\n",
    "     return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting data: {e}\")\n",
    "        return pd.DataFrame()              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4914a261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(df):\n",
    "    \"\"\"Transforms and cleans the raw job data DataFrame.\"\"\"\n",
    "    if df.empty:\n",
    "        print(\"No data to transform.\")\n",
    "        return df\n",
    "\n",
    "    # Rename columns to match database schema\n",
    "    df = df.rename(columns={\n",
    "        \"Job ID\": \"id\",\n",
    "        \"Job Title\": \"job_title\",\n",
    "        \"Company Name\": \"company_name\",\n",
    "        \"Publication Date\": \"publication_date\",\n",
    "        \"Job Type\": \"job_type\",\n",
    "        \"Category\": \"category\",\n",
    "        \"Candidate Required Location\": \"candidate_required_location\",\n",
    "        \"Salary Range\": \"salary_range\",\n",
    "        \"Job Description\": \"job_description\",\n",
    "        \"Source URL\": \"source_url\",\n",
    "        \"Company Logo\": \"company_logo\",\n",
    "        \"Job Board\": \"job_board\"\n",
    "    })\n",
    "\n",
    "    # Handle missing values\n",
    "    df = df.replace({np.nan: None})\n",
    "\n",
    "    # Convert publication_date to ISO format string\n",
    "    def parse_date(date_str):\n",
    "        if pd.isna(date_str) or date_str is None:\n",
    "            return None\n",
    "        try:\n",
    "             dt_obj = datetime.fromisoformat(date_str.replace(\"Z\", \"+00:00\"))\n",
    "             return dt_obj.isoformat()\n",
    "        except ValueError:\n",
    "                return None\n",
    "    df['publication_date'] = df['publication_date'].apply(parse_date)\n",
    "\n",
    "    # Add ingestion timestamp\n",
    "    df['ingestion_timestamp'] = datetime.now().isoformat()\n",
    "\n",
    "    # Ensure all required columns are present, fill with None if missing\n",
    "    required_cols = [\n",
    "        \"id\", \"job_title\", \"company_name\", \"publication_date\", \"job_type\",\n",
    "        \"category\", \"candidate_required_location\", \"salary_range\",\n",
    "        \"job_description\", \"source_url\", \"company_logo\", \"job_board\",\n",
    "        \"ingestion_timestamp\"\n",
    "    ]\n",
    "    for col in required_cols:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "    # Select and reorder columns to match the database schema\n",
    "    df = df[required_cols]\n",
    "\n",
    "    print(\"Data transformation complete.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ba9b008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ETL script in standalone mode.\n",
      "Successfully extracted 1559 records from remotive_jobs_extended.csv\n",
      "Data transformation complete.\n",
      "Connected to database: remote_jobs.db\n",
      "Table 'remote_jobs' ensured to exist.\n",
      "Loading 1559 records into the database...\n",
      "Successfully inserted/ignored 1559 records into 'remote_jobs'.\n",
      "Data loading complete.\n",
      "Disconnected from database.\n"
     ]
    }
   ],
   "source": [
    "def load_data(df, db_connector):\n",
    "    \"\"\"Loads the transformed DataFrame into the database using DBConnector.\"\"\"\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"No data to load.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Loading {len(df)} records into the database...\")\n",
    "    db_connector.insert_jobs(df)\n",
    "    print(\"Data loading complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Running ETL script in standalone mode.\")\n",
    "    csv_file = \"remotive_jobs_extended.csv\"\n",
    "    extracted_df = extract_data(csv_file)\n",
    "    transformed_df = transform_data(extracted_df)\n",
    "    if not transformed_df.empty:\n",
    "        db = DBConnector()\n",
    "        db.connect()\n",
    "        db.create_table()\n",
    "        load_data(transformed_df, db)\n",
    "        db.disconnect()\n",
    "    else:\n",
    "        print(\"ETL process completed with no data to load.\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206b456e",
   "metadata": {},
   "source": [
    "## 4. Utility Script (`utils.py`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7198484a",
   "metadata": {},
   "source": [
    "The `utils.py` script contains general-purpose functions that can be reused across different parts of the project. For this BI project, it includes a logging setup and a function to get the current timestamp."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0372c206",
   "metadata": {},
   "source": [
    "### Key Functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a17c31",
   "metadata": {},
   "source": [
    "- **`setup_logging(log_file, level)`**: Configures Python's `logging` module to output messages to both a file and the console. This is essential for monitoring script execution, debugging, and tracking data pipeline events.\n",
    "- **`get_current_timestamp()`**: Returns the current UTC timestamp in ISO format, useful for consistent time-stamping of data or log entries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21439504",
   "metadata": {},
   "source": [
    "### Code (`utils.py`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "297f5c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-18 19:41:31,180 - INFO - Logging is set up.\n",
      "2025-10-18 19:41:31,181 - INFO - This is a test log message.\n",
      "2025-10-18 19:41:31,181 - WARNING - This is a warning message.\n",
      "2025-10-18 19:41:31,182 - ERROR - This is an error message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current UTC Timestamp: 2025-10-18T17:41:31.184071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HELAL\\AppData\\Local\\Temp\\ipykernel_26896\\749056215.py:18: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  return datetime.utcnow().isoformat()\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "def setup_logging(log_file='app.log', level=logging.INFO):\n",
    "    \"\"\"Sets up logging configuration.\"\"\"\n",
    "    logging.basicConfig(\n",
    "        level=level,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    logging.info(\"Logging is set up.\")\n",
    "\n",
    "def get_current_timestamp():\n",
    "    \"\"\"Returns the current UTC timestamp in ISO format.\"\"\"\n",
    "    return datetime.utcnow().isoformat()    \n",
    "\n",
    "if __name__ ==  \"__main__\":\n",
    "    setup_logging()\n",
    "    logging.info(\"This is a test log message.\")\n",
    "    logging.debug(\"This is a debug message.\")\n",
    "    logging.warning(\"This is a warning message.\")\n",
    "    logging.error(\"This is an error message.\")\n",
    "    print(f\"Current UTC Timestamp: {get_current_timestamp()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5e2a6f",
   "metadata": {},
   "source": [
    "## 5. End-to-End Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff287d5",
   "metadata": {},
   "source": [
    "This section demonstrates how all the components work together. We will:\n",
    "1.  Initialize the `DBConnector` and ensure the table is created.\n",
    "2.  Run the ETL process using the `remotive_jobs_extended.csv` file.\n",
    "3.  Fetch and display a sample of the data directly from the database.\n",
    "\n",
    "> Note: Ensure `remotive_jobs_extended.csv` is present in the same directory as this notebook, or update the `csv_file` path accordingly  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100ad516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup Logging\n",
    "from utils import setup_logging\n",
    "setup_logging(log_file='etl_remotive.log', level=logging.INFO)\n",
    "logging.info(\"Starting end-to-end ETL process demonstration.\")\n",
    "\n",
    "# 2. Initialize DB Connector and Create Table\n",
    "from db_connector import DBConnector\n",
    "db = DBConnector()\n",
    "db.connect()\n",
    "db.create_table()\n",
    "\n",
    "# 3. Run ETL Process\n",
    "from etl_script import extract_data, transform_data, load_data\n",
    "csv_file = \"remotive_jobs_extended.csv\"\n",
    "extracted_df = extract_data(csv_file)\n",
    "transformed_df = transform_data(extracted_df)\n",
    "\n",
    "if not transformed_df.empty:\n",
    "    load_data(transformed_df, db)\n",
    "    logging.info(\"ETL process completed and data loaded into database.\")\n",
    "else:\n",
    "        logging.warning(\"ETL process completed with no data to load.\")\n",
    "\n",
    "# 4. Fetch and Display Data from DB\n",
    "print(\"Fetching data from database...\")\n",
    "all_jobs_df = db.fetch_all_jobs()\n",
    "if not all_jobs_from_db.empty:\n",
    "    display(Markdown(f\"### Sample of Data from Database ({len(all_jobs_from_db)} records))\"))\n",
    "    display(all_jobs_from_db.head())\n",
    "else:\n",
    "    display(Markdown(\"### No Data Found in Database\"))\n",
    "\n",
    "# 5. Disconnect from DB\n",
    "db.disconnect()\n",
    "logging.info(\"End-to-end ETL process demonstration finished\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
