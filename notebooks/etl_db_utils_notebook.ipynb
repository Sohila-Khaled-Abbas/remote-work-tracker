{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remote Work Tracker BI Project: ETL, DB, and Utility Scripts\n",
    "\n",
    "This Jupyter Notebook provides a comprehensive overview and demonstration of the Extract, Transform, Load (ETL) process, database interaction, and utility functions developed for the Remote Work Tracker Business Intelligence (BI) project. These scripts are designed to process raw job data (e.g., from the Remotive.com API) and store it in a structured SQLite database, making it ready for further analysis and visualization in tools like Power BI.\n",
    "\n",
    "## Project Components\n",
    "\n",
    "The core components covered in this notebook are:\n",
    "1.  **`db_schema_and_etl_design.md`**: Documentation outlining the database schema and ETL process.\n",
    "2.  **`db_connector.py`**: Python script for connecting to and interacting with the SQLite database.\n",
    "3.  **`etl_script.py`**: Python script implementing the Extract, Transform, Load logic.\n",
    "4.  **`utils.py`**: Python script containing general utility functions, such as logging.\n",
    "\n",
    "## 1. Database Schema Design\n",
    "\n",
    "The foundation of our BI project is a well-defined database schema. We are using a simple SQLite database with a single table, `remote_jobs`, to store the processed job data. The schema is designed to capture all relevant information from the job postings.\n",
    "\n",
    "### `remote_jobs` Table Structure\n",
    "\n",
    "| Column Name                   | Data Type   | Constraints       | Description                                      |\n",
    "| :---------------------------- | :---------- | :---------------- | :----------------------------------------------- |\n",
    "| `id`                          | INTEGER     | PRIMARY KEY       | Unique identifier for each job posting (from API) |\n",
    "| `job_title`                   | TEXT        | NOT NULL          | Title of the job                                 |\n",
    "| `company_name`                | TEXT        | NOT NULL          | Name of the hiring company                       |\n",
    "| `publication_date`            | TEXT        | NOT NULL          | Date and time the job was published (ISO format) |\n",
    "| `job_type`                    | TEXT        |                   | Type of employment (e.g., full_time, contract)   |\n",
    "| `category`                    | TEXT        |                   | Job category (e.g., Software Development)        |\n",
    "| `candidate_required_location` | TEXT        |                   | Geographical restrictions for candidates         |\n",
    "| `salary_range`                | TEXT        |                   | Stated salary range, if available                |\n",
    "| `job_description`             | TEXT        |                   | Full description of the job                      |\n",
    "| `source_url`                  | TEXT        | UNIQUE, NOT NULL  | URL to the original job posting                  |\n",
    "| `company_logo`                | TEXT        |                   | URL to the company logo                          |\n",
    "| `job_board`                   | TEXT        | NOT NULL          | Source job board (e.g., Remotive.com)            |\n",
    "| `ingestion_timestamp`         | TIMESTAMP   | DEFAULT CURRENT_TIMESTAMP | Timestamp when the record was ingested           |\n",
    "\n",
    "*Note: The `db_schema_and_etl_design.md` file contains a more detailed explanation of the schema and ETL process.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Database Connector (`db_connector.py`)\n",
    "\n",
    "The `db_connector.py` script provides a `DBConnector` class to manage interactions with the SQLite database. It handles connecting, disconnecting, creating the `remote_jobs` table, and inserting job data from a Pandas DataFrame.\n",
    "\n",
    "### Key Features:\n",
    "*   **Connection Management**: `connect()` and `disconnect()` methods for robust database handling.\n",
    "*   **Schema Initialization**: `create_table()` ensures the `remote_jobs` table exists with the defined schema.\n",
    "*   **Data Insertion**: `insert_jobs(df)` efficiently inserts DataFrame records, using `INSERT OR IGNORE` to prevent duplicate entries based on the `id` column (which is the primary key from the API). This is crucial for handling daily scrapes without re-inserting old data.\n",
    "*   **Data Retrieval**: `fetch_all_jobs()` allows for easy retrieval of all stored job data into a Pandas DataFrame for analysis.\n",
    "\n",
    "### Code (`db_connector.py`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "class DBConnector:\n",
    "    def __init__(self, db_name=\"remote_jobs.db\"):\n",
    "        self.db_name = db_name\n",
    "        self.conn = None\n",
    "        self.cursor = None\n",
    "\n",
    "    def connect(self):\n",
    "        \"\"\"Establishes a connection to the SQLite database.\"\"\"\n",
    "        try:\n",
    "            self.conn = sqlite3.connect(self.db_name)\n",
    "            self.cursor = self.conn.cursor()\n",
    "            print(f\"Connected to database: {self.db_name}\")\n",
    "        except sqlite3.Error as e:\n",
    "            print(f\"Error connecting to database: {e}\")\n",
    "\n",
    "    def disconnect(self):\n",
    "        \"\"\"Closes the database connection.\"\"\"\n",
    "        if self.conn:\n",
    "            self.conn.close()\n",
    "            print(\"Disconnected from database.\")\n",
    "\n",
    "    def create_table(self):\n",
    "        \"\"\"Creates the remote_jobs table if it doesn\"t exist.\"\"\"\n",
    "        if not self.conn:\n",
    "            self.connect()\n",
    "        \n",
    "        create_table_sql = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS remote_jobs (\n",
    "            id INTEGER PRIMARY KEY,\n",
    "            job_title TEXT NOT NULL,\n",
    "            company_name TEXT NOT NULL,\n",
    "            publication_date TEXT NOT NULL,\n",
    "            job_type TEXT,\n",
    "            category TEXT,\n",
    "            candidate_required_location TEXT,\n",
    "            salary_range TEXT,\n",
    "            job_description TEXT,\n",
    "            source_url TEXT UNIQUE NOT NULL,\n",
    "            company_logo TEXT,\n",
    "            job_board TEXT NOT NULL,\n",
    "            ingestion_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        );\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.cursor.execute(create_table_sql)\n",
    "            self.conn.commit()\n",
    "            print(\"Table \"\"remote_jobs\"\" ensured to exist.\")\n",
    "        except sqlite3.Error as e:\n",
    "            print(f\"Error creating table: {e}\")\n",
    "\n",
    "    def insert_jobs(self, df: pd.DataFrame):\n",
    "        \"\"\"Inserts job data from a Pandas DataFrame into the remote_jobs table.\n",
    "           Handles duplicates by ignoring entries with existing source_url.\n",
    "        \"\"\"\n",
    "        if df.empty:\n",
    "            print(\"No data to insert.\")\n",
    "            return\n",
    "\n",
    "        if not self.conn:\n",
    "            self.connect()\n",
    "            self.create_table() # Ensure table exists before inserting\n",
    "\n",
    "        data_to_insert = []\n",
    "        for index, row in df.iterrows():\n",
    "            data_to_insert.append((\n",
    "                row.get(\"id\"),\n",
    "                row.get(\"job_title\"),\n",
    "                row.get(\"company_name\"),\n",
    "                row.get(\"publication_date\"),\n",
    "                row.get(\"job_type\"),\n",
    "                row.get(\"category\"),\n",
    "                row.get(\"candidate_required_location\"),\n",
    "                row.get(\"salary_range\"),\n",
    "                row.get(\"job_description\"),\n",
    "                row.get(\"source_url\"),\n",
    "                row.get(\"company_logo\"),\n",
    "                row.get(\"job_board\"),\n",
    "                row.get(\"ingestion_timestamp\", datetime.now().isoformat()) \n",
    "            ))\n",
    "\n",
    "        insert_sql = \"\"\"\n",
    "        INSERT OR IGNORE INTO remote_jobs (\n",
    "            id, job_title, company_name, publication_date, job_type, category,\n",
    "            candidate_required_location, salary_range, job_description, source_url,\n",
    "            company_logo, job_board, ingestion_timestamp\n",
    "        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?);\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.cursor.executemany(insert_sql, data_to_insert)\n",
    "            self.conn.commit()\n",
    "            print(f\"Successfully inserted/ignored {len(data_to_insert)} records into \"\"remote_jobs\"\".\")\n",
    "        except sqlite3.Error as e:\n",
    "            print(f\"Error inserting data: {e}\")\n",
    "\n",
    "    def fetch_all_jobs(self):\n",
    "        \"\"\"Fetches all job records from the database.\"\"\"\n",
    "        if not self.conn:\n",
    "            self.connect()\n",
    "\n",
    "        try:\n",
    "            self.cursor.execute(\"SELECT * FROM remote_jobs;\")\n",
    "            columns = [description[0] for description in self.cursor.description]\n",
    "            rows = self.cursor.fetchall()\n",
    "            df = pd.DataFrame(rows, columns=columns)\n",
    "            return df\n",
    "        except sqlite3.Error as e:\n",
    "            print(f\"Error fetching data: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    db = DBConnector()\n",
    "    db.connect()\n",
    "    db.create_table()\n",
    "\n",
    "    # Example of inserting dummy data\n",
    "    dummy_data = pd.DataFrame([\n",
    "        {\n",
    "            \"id\": 1,\n",
    "            \"job_title\": \"Software Engineer\",\n",
    "            \"company_name\": \"Tech Corp\",\n",
    "            \"publication_date\": \"2025-10-15T10:00:00\",\n",
    "            \"job_type\": \"full_time\",\n",
    "            \"category\": \"Software Development\",\n",
    "            \"candidate_required_location\": \"Worldwide\",\n",
    "            \"salary_range\": \"$80,000 - $120,000\",\n",
    "            \"job_description\": \"Develop and maintain software.\",\n",
    "            \"source_url\": \"http://example.com/job1\",\n",
    "            \"company_logo\": \"http://example.com/logo1.png\",\n",
    "            \"job_board\": \"ExampleJobs\",\n",
    "            \"ingestion_timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        {\n",
    "            \"id\": 2,\n",
    "            \"job_title\": \"Data Analyst\",\n",
    "            \"company_name\": \"Data Insights Inc.\",\n",
    "            \"publication_date\": \"2025-10-14T11:30:00\",\n",
    "            \"job_type\": \"contract\",\n",
    "            \"category\": \"Data Analysis\",\n",
    "            \"candidate_required_location\": \"Remote US\",\n",
    "            \"salary_range\": \"$50/hr - $70/hr\",\n",
    "            \"job_description\": \"Analyze large datasets.\",\n",
    "            \"source_url\": \"http://example.com/job2\",\n",
    "            \"company_logo\": \"http://example.com/logo2.png\",\n",
    "            \"job_board\": \"ExampleJobs\",\n",
    "            \"ingestion_timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "    ])\n",
    "    db.insert_jobs(dummy_data)\n",
    "\n",
    "    all_jobs = db.fetch_all_jobs()\n",
    "    print(\"\\nAll jobs in DB:\")\n",
    "    print(all_jobs.head())\n",
    "\n",
    "    db.disconnect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ETL Script (`etl_script.py`)\n",
    "\n",
    "The `etl_script.py` orchestrates the data pipeline, performing the Extract, Transform, and Load operations. It reads raw data, cleans and standardizes it, and then uses the `DBConnector` to persist it in the database.\n",
    "\n",
    "### Key Functions:\n",
    "*   **`extract_data(file_path)`**: Reads the raw job data from a specified CSV file into a Pandas DataFrame. This acts as the \"Extract\" stage.\n",
    "*   **`transform_data(df)`**: Cleans and transforms the DataFrame. This includes renaming columns to match the database schema, handling missing values (replacing `NaN` with `None`), converting `publication_date` to a consistent ISO format, and adding an `ingestion_timestamp`. This is the \"Transform\" stage.\n",
    "*   **`load_data(df, db_connector)`**: Takes the transformed DataFrame and an instance of `DBConnector` to insert the data into the `remote_jobs` table. This is the \"Load\" stage.\n",
    "\n",
    "### Code (`etl_script.py`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from db_connector import DBConnector # Import the DBConnector\n",
    "\n",
    "def extract_data(file_path):\n",
    "    \"\"\"Extracts data from a CSV file.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Successfully extracted {len(df)} records from {file_path}\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during data extraction: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def transform_data(df):\n",
    "    \"\"\"Transforms the extracted job data.\"\"\"\n",
    "    if df.empty:\n",
    "        print(\"No data to transform.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(\"Starting data transformation...\")\n",
    "\n",
    "    # Rename columns for consistency with DB schema\n",
    "    df = df.rename(columns={\n",
    "        \"Job ID\": \"id\",\n",
    "        \"Job Title\": \"job_title\",\n",
    "        \"Company Name\": \"company_name\",\n",
    "        \"Publication Date\": \"publication_date\",\n",
    "        \"Job Type\": \"job_type\",\n",
    "        \"Category\": \"category\",\n",
    "        \"Candidate Required Location\": \"candidate_required_location\",\n",
    "        \"Salary Range\": \"salary_range\",\n",
    "        \"Job Description\": \"job_description\",\n",
    "        \"Source URL\": \"source_url\",\n",
    "        \"Company Logo\": \"company_logo\",\n",
    "        \"Job Board\": \"job_board\"\n",
    "    })\n",
    "\n",
    "    # Handle missing values: Replace NaN with None for database insertion\n",
    "    df = df.replace({np.nan: None})\n",
    "\n",
    "    # Convert publication_date to datetime objects and then to ISO format string\n",
    "    def parse_date(date_str):\n",
    "        if pd.isna(date_str) or date_str is None:\n",
    "            return None\n",
    "        try:\n",
    "            dt_obj = datetime.fromisoformat(date_str.replace(\'Z\', \'+00:00\'))\n",
    "            return dt_obj.isoformat()\n",
    "        except ValueError:\n",
    "            return None\n",
    "\n",
    "    df[\"publication_date\"] = df[\"publication_date\"].apply(parse_date)\n",
    "\n",
    "    # Add ingestion timestamp\n",
    "    df[\"ingestion_timestamp\"] = datetime.now().isoformat()\n",
    "\n",
    "    # Ensure all required columns are present, fill with None if missing\n",
    "    required_cols = [\n",
    "        \"id\", \"job_title\", \"company_name\", \"publication_date\", \"job_type\",\n",
    "        \"category\", \"candidate_required_location\", \"salary_range\", \"job_description\",\n",
    "        \"source_url\", \"company_logo\", \"job_board\", \"ingestion_timestamp\"\n",
    "    ]\n",
    "    for col in required_cols:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "\n",
    "    # Select and reorder columns to match the database schema\n",
    "    df = df[required_cols]\n",
    "\n",
    "    print(\"Data transformation complete.\")\n",
    "    return df\n",
    "\n",
    "def load_data(df, db_connector):\n",
    "    \"\"\"Loads the transformed data into the database using a DB connector.\"\"\"\n",
    "    if df.empty:\n",
    "        print(\"No data to load.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Starting data loading for {len(df)} records...\")\n",
    "    db_connector.insert_jobs(df)\n",
    "    print(\"Data loading complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Running ETL script in standalone mode.\")\n",
    "    \n",
    "    csv_file = \"remotive_jobs_extended.csv\"\n",
    "    extracted_df = extract_data(csv_file)\n",
    "    transformed_df = transform_data(extracted_df)\n",
    "\n",
    "    if not transformed_df.empty:\n",
    "        db = DBConnector()\n",
    "        db.connect()\n",
    "        db.create_table()\n",
    "        load_data(transformed_df, db)\n",
    "        db.disconnect()\n",
    "    else:\n",
    "        print(\"ETL process completed with no data to load.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Utility Script (`utils.py`)\n",
    "\n",
    "The `utils.py` script contains general-purpose functions that can be reused across different parts of the project. For this BI project, it includes a logging setup and a function to get the current timestamp.\n",
    "\n",
    "### Key Functions:\n",
    "*   **`setup_logging(log_file, level)`**: Configures Python's `logging` module to output messages to both a file and the console. This is essential for monitoring script execution, debugging, and tracking data pipeline events.\n",
    "*   **`get_current_timestamp()`**: Returns the current UTC timestamp in ISO format, useful for consistent time-stamping of data or log entries.\n",
    "\n",
    "### Code (`utils.py`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "def setup_logging(log_file=\"app.log\", level=logging.INFO):\n",
    "    \"\"\"Sets up basic logging for the application.\"\"\"\n",
    "    logging.basicConfig(\n",
    "        level=level,\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    logging.info(\"Logging setup complete.\")\n",
    "\n",
    "def get_current_timestamp():\n",
    "    \"\"\"Returns the current UTC timestamp in ISO format.\"\"\"\n",
    "    return datetime.utcnow().isoformat()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    setup_logging(\"test_app.log\", logging.DEBUG)\n",
    "    logging.debug(\"This is a debug message.\")\n",
    "    logging.info(\"This is an info message.\")\n",
    "    logging.warning(\"This is a warning message.\")\n",
    "    logging.error(\"This is an error message.\")\n",
    "    print(f\"Current timestamp: {get_current_timestamp()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. End-to-End Demonstration\n",
    "\n",
    "This section demonstrates how all the components work together. We will:\n",
    "1.  Initialize the `DBConnector` and ensure the table is created.\n",
    "2.  Run the ETL process using the `remotive_jobs_extended.csv` file.\n",
    "3.  Fetch and display a sample of the data directly from the database.\n",
    "\n",
    "*Note: Ensure `remotive_jobs_extended.csv` is present in the same directory as this notebook, or update the `csv_file` path accordingly.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup Logging\n",
    "from utils import setup_logging\n",
    "setup_logging(log_file=\"etl_process.log\")\n",
    "logging.info(\"Starting end-to-end ETL process demonstration.\")\n",
    "\n",
    "# 2. Initialize DB Connector and Create Table\n",
    "from db_connector import DBConnector\n",
    "db = DBConnector()\n",
    "db.connect()\n",
    "db.create_table()\n",
    "\n",
    "# 3. Run ETL Process\n",
    "from etl_script import extract_data, transform_data, load_data\n",
    "csv_file = \"remotive_jobs_extended.csv\"\n",
    "\n",
    "extracted_df = extract_data(csv_file)\n",
    "transformed_df = transform_data(extracted_df)\n",
    "\n",
    "if not transformed_df.empty:\n",
    "    load_data(transformed_df, db)\n",
    "    logging.info(\"ETL process completed and data loaded into database.\")\n",
    "else:\n",
    "    logging.warning(\"ETL process completed with no data to load.\")\n",
    "\n",
    "# 4. Fetch and Display Data from DB\n",
    "print(\"\\nFetching data from database...\")\n",
    "all_jobs_from_db = db.fetch_all_jobs()\n",
    "if not all_jobs_from_db.empty:\n",
    "    display(Markdown(f\"### Sample of Data from Database ({len(all_jobs_from_db)} records)\"))\n",
    "    display(all_jobs_from_db.head())\n",
    "else:\n",
    "    display(Markdown(\"### No Data Found in Database\"))\n",
    "\n",
    "# 5. Disconnect from DB\n",
    "db.disconnect()\n",
    "logging.info(\"End-to-end ETL process demonstration finished.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
